# Medical Coding RAG System - Implementation Specification

## Project Overview

Build a production-ready Medical Coding Assistant using RAG (Retrieval Augmented Generation) that maps clinical descriptions to ICD-10-CM diagnosis codes and CPT procedure codes.

**Target User**: Data analyst transitioning to AI/ML engineering  
**Purpose**: Portfolio project demonstrating full-stack AI engineering, system design, and healthcare domain knowledge  
**Data Source**: `fictional_cpt_to_icd10_mapping.csv` (81,393 mappings, 10,900 unique CPT codes)

---

## Tech Stack Requirements

### Backend (MANDATORY)
- **Framework**: FastAPI (Python 3.10+)
- **Validation**: Pydantic for request/response models
- **Database**: Neon.tech (Postgres with pgvector extension)
- **Embeddings**: sentence-transformers (locally run, no API cost)
- **LLM**: Perplexity API (user has API key)
- **Connection**: asyncpg for async Postgres queries

### Frontend (MANDATORY)
- **Framework**: Next.js 14+ with App Router
- **Language**: TypeScript (strict mode)
- **Styling**: Tailwind CSS
- **HTTP Client**: Axios with interceptors
- **State Management**: React hooks + Context API (or React Query)

### Infrastructure
- **API Docs**: OpenAPI/Swagger (auto-generated by FastAPI)
- **Vector Search**: pgvector extension in Postgres
- **Deployment**: Local first, Docker containerization for portability

---

## System Architecture

### Data Flow (Critical to Understand)

```
User Input → FastAPI Endpoint → Generate Embedding → Query Neon (pgvector) 
→ Retrieve Top 20 Codes → Send to Perplexity LLM → Rank & Explain 
→ Return JSON → Next.js Display
```

### Why This Architecture?

**Neon + pgvector Instead of Dedicated Vector DB**:
- Single database for structured data AND vectors
- Leverage existing SQL knowledge
- Enable hybrid queries (filter by category + vector search)
- Serverless scaling without ops overhead
- Cost-effective for portfolio projects

**Embeddings Generated Locally**:
- No API cost per query
- Fast inference (~50ms on CPU)
- Privacy (no data sent to third parties)
- Upgrade path to BioBERT later

**Perplexity for LLM Reasoning**:
- Better than pure vector search (provides explanations)
- Online mode can reference current medical guidelines
- OpenAI-compatible API (easy integration)
- Cost-effective compared to GPT-4

---

## Mandatory Backend Implementation

### 1. Project Structure (Non-Negotiable)

```
backend/
├── app/
│   ├── main.py              # FastAPI app, CORS, startup/shutdown
│   ├── config.py            # Pydantic BaseSettings for env vars
│   ├── database.py          # Neon connection pool (asyncpg)
│   ├── models/
│   │   ├── request_models.py   # Pydantic schemas for requests
│   │   └── response_models.py  # Pydantic schemas for responses
│   ├── services/
│   │   ├── embeddings.py       # sentence-transformers wrapper
│   │   ├── vector_search.py    # pgvector queries
│   │   ├── llm_service.py      # Perplexity API client
│   │   └── code_ranker.py      # Post-processing logic
│   └── utils/
│       ├── data_loader.py      # CSV → Postgres with embeddings
│       ├── logger.py           # Structured logging
│       └── exceptions.py       # Custom exception classes
├── scripts/
│   └── setup_database.py    # Initialize Neon DB schema
├── data/
│   └── fictional_cpt_to_icd10_mapping.csv
├── requirements.txt
├── .env.example
└── Dockerfile
```

### 2. Database Schema (MUST Implement Exactly)

**ALTERNATIVE SCHEMA (for separate CPT and ICD-10 files):**

Instead of a single table, use TWO tables:

**Table: cpt_codes**
- `id` (SERIAL PRIMARY KEY)
- `cpt_code` (VARCHAR(5))
- `cpt_description` (TEXT)
- `category` (VARCHAR(50))
- `embedding` (vector(384)) ← pgvector type
- `created_at` (TIMESTAMP)

**Table: icd10_codes**
- `id` (SERIAL PRIMARY KEY)
- `icd10_code` (VARCHAR(10))
- `icd10_description` (TEXT)
- `embedding` (vector(384)) ← pgvector type
- `created_at` (TIMESTAMP)

**Index** (for EACH table):
```sql
CREATE INDEX ON cpt_codes USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX ON icd10_codes USING ivfflat (embedding vector_cosine_ops);
```

**ORIGINAL SCHEMA** (if you have the mapping file):
- `id` (SERIAL PRIMARY KEY)
- `cpt_code` (VARCHAR(5))
- `cpt_description` (TEXT)
- `icd10_code` (VARCHAR(10))
- `icd10_description` (TEXT)
- `category` (VARCHAR(50))
- `embedding` (vector(384)) ← pgvector type
- `created_at` (TIMESTAMP)

**Index** (Critical for Performance):
```sql
CREATE INDEX ON medical_codes USING ivfflat (embedding vector_cosine_ops);
```

**Why IVFFlat Index?**
- Approximate nearest neighbor search (10-100x faster than exact)
- Suitable for 10K-1M vectors
- Uses cosine distance operator `<=>`
- Must be created AFTER data is loaded

### 3. Embedding Generation Service (MUST Have)

**Model Selection**:
- **MVP**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)
- **Future**: `dmis-lab/biobert-base-cased-v1.2` (768 dimensions, medical domain)

**Critical Requirements**:
- Load model once at startup (singleton pattern)
- Batch processing for data loading (batch_size=32-100)
- L2 normalization for cosine similarity
- Cache embeddings to avoid recomputation

**What to Implement**:
- Function: `generate_embeddings(texts: List[str]) -> np.ndarray`
- Function: `embed_clinical_query(query: str) -> np.ndarray`
- Model loaded in memory (don't reload per request)

### 4. Vector Search Service (MUST Implement)

**Query Pattern (for original single-table schema)**:
```sql
SELECT *, 1 - (embedding <=> $1) as similarity_score
FROM medical_codes
WHERE category = $2 OR $2 IS NULL
ORDER BY embedding <=> $1
LIMIT $3
```

**Query Pattern (for separate tables approach)**:
Run TWO separate queries:

```sql
-- Query 1: CPT codes
SELECT 'CPT' as code_type, cpt_code, cpt_description, category, 1 - (embedding <=> $1) as similarity_score
FROM cpt_codes
ORDER BY embedding <=> $1
LIMIT $3

-- Query 2: ICD-10 codes
SELECT 'ICD10' as code_type, icd10_code, icd10_description, '' as category, 1 - (embedding <=> $1) as similarity_score
FROM icd10_codes
ORDER BY embedding <=> $1
LIMIT $3
```

Then combine results and send top 20 total to LLM for ranking.

**Critical Details**:
- `<=>` is cosine distance operator (0 = identical, 2 = opposite)
- Convert to similarity: `1 - distance` for intuitive scores
- Retrieve top 20 results (more than final output for reranking)
- Optional category filtering
- Return results with similarity scores

**What to Implement**:
- Async function using asyncpg connection pool
- Parameterized queries (prevent SQL injection)
- Error handling for connection failures
- Fallback to broader search if no results

### 5. Perplexity LLM Service (MUST Implement)

**API Integration**:
- Use OpenAI Python SDK (Perplexity is compatible)
- Base URL: `https://api.perplexity.ai`
- Model: `llama-3.1-sonar-large-128k-online` or `sonar-small-128k-online`

**Prompt Engineering Requirements**:
- **System Prompt**: Establish medical coding expert persona
- **Context Injection**: Include retrieved codes with similarity scores
- **Task Instructions**: Rank codes, assign confidence, explain reasoning
- **Output Format**: Enforce JSON structure
- **Temperature**: 0.2 (low for consistency)
- **Max Tokens**: 2000 (balance cost and detail)

**Critical Functionality**:
- Retry logic with exponential backoff (network failures)
- Timeout handling (30 second max)
- Fallback to rule-based ranking if LLM fails
- JSON validation before returning

**What to Implement**:
- Async function: `generate_code_suggestions(clinical_text, retrieved_codes)`
- Parse LLM JSON response
- Validate structure matches Pydantic models
- Implement fallback ranking using similarity scores

### 6. Pydantic Models (MUST Define)

**Request Model**:
```python
CodingQuery:
  - clinical_description: str (min 10 chars, max 2000 chars)
  - max_results: int (default 5, range 1-20)
  - include_explanation: bool (default True)
  - filter_category: Optional[str]
```

**Response Model**:
```python
CodeSuggestion:
  - code: str
  - code_type: Literal["CPT", "ICD-10"]
  - description: str
  - category: str
  - confidence_score: float (0.0-1.0)
  - similarity_score: float
  - reasoning: Optional[str]

CodingResponse:
  - query: str
  - cpt_codes: List[CodeSuggestion]
  - icd10_codes: List[CodeSuggestion]
  - explanation: Optional[str]
  - processing_time_ms: float
```

### 7. FastAPI Endpoints (MUST Implement)

1. **POST /api/code-suggestions**
   - Main endpoint for code suggestions
   - Accepts CodingQuery model
   - Returns CodingResponse model
   - Validates with Pydantic
   - Auto-documented in Swagger

2. **GET /api/stats**
   - Return dataset statistics
   - Total records, unique codes, category distribution

3. **GET /health**
   - Health check endpoint
   - Test database connection
   - Return service status

4. **GET /docs** (Automatic)
   - Swagger UI (auto-generated by FastAPI)

5. **GET /openapi.json** (Automatic)
   - OpenAPI schema

### 8. Configuration Management (MUST Implement)

**Environment Variables Required**:
- `NEON_DATABASE_URL` (Postgres connection string)
- `PERPLEXITY_API_KEY` (LLM API key)
- `EMBEDDING_MODEL_NAME` (default: all-MiniLM-L6-v2)
- `LOG_LEVEL` (default: INFO)
- `CORS_ORIGINS` (default: http://localhost:3000)

**Use Pydantic BaseSettings**:
- Type validation for config
- Auto-load from .env file
- Fail fast if required vars missing

### 9. Data Loading Script (MUST Create)

**Script: setup_database.py**

**Responsibilities (for original mapping file)**:
1. Connect to Neon database
2. Enable pgvector extension
3. Create medical_codes table
4. Load CSV file
5. Generate embeddings for each row (batch process)
6. Insert data with embeddings
7. Create IVFFlat index
8. Print statistics

**Responsibilities (for separate CPT and ICD-10 files)**:
1. Connect to Neon database
2. Enable pgvector extension
3. Create **TWO tables**: `cpt_codes` and `icd10_codes`
4. Load `all-2025-cpt-codes.csv` → Parse and create embedding from "Procedure Code Descriptions"
5. Load `icd10cm-codes-2025.txt` → Parse code and description, create embeddings
6. Generate embeddings for each row (batch process) 
7. Insert data with embeddings into respective tables
8. Create IVFFlat index on BOTH tables
9. Print statistics for each table

**Critical Requirements**:
- Show progress bar (tqdm)
- Batch embedding generation (don't OOM)
- Bulk insert for speed
- Handle duplicates gracefully
- Log any errors
- For CPT file: Extract category from first column for filtering
- For ICD-10 file: Parse space-delimited format (code + description)

**Expected Runtime**: 
- 5-10 minutes for 81K records (mapping file approach)
- 10-15 minutes for separate files (1,164 CPT + 74K ICD-10 codes)

---

## Mandatory Frontend Implementation

### 1. Project Structure (Non-Negotiable)

```
frontend/
├── src/
│   ├── app/
│   │   ├── page.tsx           # Main landing page
│   │   ├── layout.tsx         # Root layout
│   │   └── globals.css
│   ├── components/
│   │   ├── CodingAssistant.tsx    # Main search interface
│   │   ├── QueryInput.tsx         # Textarea with validation
│   │   ├── ResultsDisplay.tsx     # Results container
│   │   ├── CodeCard.tsx           # Individual code card
│   │   ├── ConfidenceBar.tsx      # Visual confidence score
│   │   └── LoadingState.tsx       # Skeleton loaders
│   ├── lib/
│   │   ├── api.ts             # Axios API client
│   │   └── types.ts           # TypeScript interfaces
│   └── hooks/
│       └── useCodingSuggestions.ts
├── .env.local.example
└── package.json
```

### 2. API Client (MUST Implement)

**Requirements**:
- Axios instance with base URL configuration
- Request/response interceptors
- Type-safe interfaces matching backend Pydantic models
- Error handling with user-friendly messages
- 30 second timeout
- Retry logic for network failures

**What to Implement**:
- Function: `getCodingSuggestions(query: CodingQuery): Promise<CodingResponse>`
- Function: `getStatistics(): Promise<Stats>`
- Error formatting for display
- Loading state management

### 3. Main UI Components (MUST Build)

**CodingAssistant Component**:
- Textarea for clinical description (4-6 rows)
- Character counter
- Submit button with loading state
- Example queries (clickable pills)
- Results display area
- Error message display

**CodeCard Component**:
- Display code number prominently
- Show code type badge (CPT vs ICD-10)
- Description text
- Category label
- Confidence score visualization
- Expandable reasoning section
- Copy to clipboard button

**ConfidenceBar Component**:
- Visual bar or circle showing confidence
- Color-coded: Green (>80%), Yellow (60-80%), Orange (<60%)
- Percentage display
- Tooltip with explanation

### 4. TypeScript Interfaces (MUST Define)

Mirror backend Pydantic models exactly:
- `CodingQuery` interface
- `CodeSuggestion` interface
- `CodingResponse` interface

Use strict TypeScript mode (no `any` types).

### 5. UI/UX Requirements (MUST Have)

**Responsive Design**:
- Mobile-first approach
- Works on 320px width minimum
- Tablet and desktop optimized

**Loading States**:
- Skeleton loaders during API call
- Disable input during loading
- Show processing time after completion

**Error Handling**:
- Display user-friendly error messages
- Distinguish network vs validation errors
- "Try again" button for failures

**Accessibility**:
- Proper ARIA labels
- Keyboard navigation support
- Focus management

**Visual Design**:
- Clean, professional medical app aesthetic
- Tailwind CSS for styling
- Consistent spacing and typography
- Prominent disclaimer about fictional data

---

## Code Quality Standards (NON-NEGOTIABLE)

### 1. Comments and Documentation

**Every Function Must Have**:
- Docstring explaining purpose
- Parameter descriptions
- Return value description
- Implementation notes explaining WHY

**Inline Comments Must Explain**:
- Design decisions and trade-offs
- Performance considerations
- Edge cases handled
- Future optimization opportunities
- Why certain libraries/approaches chosen

**Comment Style**:
- Not AI-generated looking (vary style)
- Include realistic TODOs
- Reference domain knowledge (medical coding standards)
- Explain business logic, not syntax

### 2. Error Handling

**Backend Requirements**:
- Custom exception classes (not generic Exception)
- Try-except blocks around external calls
- Structured logging (not print statements)
- Return meaningful error messages
- HTTP status codes correctly used

**Frontend Requirements**:
- Catch all API errors
- Display user-friendly messages
- Log technical details to console
- Graceful degradation

### 3. Type Safety

**Backend**:
- Type hints on all functions
- Pydantic models for validation
- No untyped dictionaries for data

**Frontend**:
- Strict TypeScript mode
- No `any` types
- Interfaces for all data structures

### 4. Performance

**Backend**:
- Connection pooling (don't create new connections per request)
- Async operations where possible
- Batch processing for bulk operations
- Index usage in database queries

**Frontend**:
- Debounce user input
- Loading states prevent double-submission
- Optimize re-renders

### 5. Security

**Backend**:
- Parameterized SQL queries
- CORS properly configured
- No secrets in code
- Input validation via Pydantic

**Frontend**:
- Environment variables for config
- No API keys in client code
- Sanitize user input before display

---

## Documentation Requirements (MUST CREATE)

### 1. README.md (Comprehensive)

**Must Include**:
- Project overview (1 paragraph)
- Architecture diagram (ASCII or description)
- Technology stack with justifications
- Setup instructions (step-by-step)
- Environment variables explained
- How to run locally
- API documentation link
- Performance metrics
- Future enhancements roadmap

**Architecture Section Must Explain**:
- Why RAG over fine-tuning
- Why Neon + pgvector over dedicated vector DB
- Why local embeddings + cloud LLM
- Data flow from query to response
- Component interactions

### 2. ARCHITECTURE.md (Deep Dive)

**Must Document**:
- System design rationale
- Component breakdown
- Database schema with ER diagram
- Vector search strategy
- Prompt engineering approach
- Error handling patterns
- Scaling considerations
- Performance bottlenecks and solutions

### 3. API_DOCUMENTATION.md

**Must Include**:
- All endpoint descriptions
- Request/response examples (JSON)
- Error codes and meanings
- Swagger UI location
- Example curl commands

### 4. DEPLOYMENT.md

**Must Cover**:
- Local development setup
- Docker deployment steps
- Environment variables for production
- Recommended cloud platforms
- Cost estimation
- Monitoring setup

### 5. INTERVIEW_PREP.md

**Must Include Technical Q&A**:

**System Design Questions**:
- "Why RAG architecture for this problem?"
- "How does pgvector similarity search work?"
- "Explain your database schema design"
- "How would you scale to 10M records?"
- "How would you reduce latency?"

**Implementation Questions**:
- "Walk me through the request flow"
- "How do you handle API failures?"
- "Explain Pydantic validation strategy"
- "How do you prevent SQL injection?"

**Domain Questions**:
- "What's the difference between ICD-10 and CPT?"
- "Why are multiple diagnoses mapped to one procedure?"
- "How do medical coders use these systems?"

**Performance Questions**:
- "Where are the bottlenecks?"
- "How would you implement caching?"
- "What's your monitoring strategy?"

### 6. EVALUATION.md

**Must Document**:
- Metrics used (Precision@K, Recall@K)
- How to create test datasets
- Evaluation methodology
- Baseline comparisons
- Error analysis approach

### 7. PROMPT_ENGINEERING.md

**Must Explain**:
- System prompt design rationale
- Context formatting strategy
- Why specific temperature chosen
- Output format enforcement method
- Fallback strategies
- Prompt versioning approach

---

## Testing and Validation (MUST DO)

### 1. Backend Testing

**Setup pytest structure** (don't need to write all tests, but structure must exist):
- `tests/` directory
- `conftest.py` with fixtures
- Test files for each service
- Mock Perplexity API responses
- Mock database queries

### 2. Manual Testing Scenarios

**Must Test These Flows**:
1. Simple query: "Type 2 diabetes"
2. Complex query: "Patient with chest pain, hypertension, and shortness of breath"
3. Category-specific query
4. Very short query (edge case)
5. Very long query (edge case)
6. Invalid input (empty, special characters)
7. API failure simulation
8. Database connection failure

### 3. Performance Benchmarks

**Must Measure**:
- End-to-end latency (target: <2.5 seconds)
- Embedding generation time
- Vector search time
- LLM API call time
- Memory usage
- Database query performance

---

## Development Workflow (RECOMMENDED)

### Phase 1: Database Setup (Day 1)
1. Set up Neon account and create database
2. Enable pgvector extension
3. Create schema
4. Write data loader script
5. Load CSV with embeddings
6. Verify vector search works

### Phase 2: Backend Core (Day 2-3)
1. Set up FastAPI project structure
2. Implement Pydantic models
3. Create embedding service
4. Create vector search service
5. Test basic retrieval

### Phase 3: LLM Integration (Day 4)
1. Set up Perplexity API client
2. Design prompts
3. Implement LLM service
4. Test with retrieved results
5. Add fallback ranking

### Phase 4: Backend Completion (Day 5)
1. Wire up all endpoints
2. Add error handling
3. Add logging
4. Test end-to-end
5. Document in Swagger

### Phase 5: Frontend (Day 6-7)
1. Set up Next.js project
2. Build API client
3. Create UI components
4. Integrate with backend
5. Add loading/error states
6. Polish UI/UX

### Phase 6: Documentation (Day 8)
1. Write all required .md files
2. Add code comments
3. Create examples
4. Test setup instructions

### Phase 7: Polish (Day 9-10)
1. Docker setup
2. Performance optimization
3. Security review
4. Interview prep materials
5. Final testing

---

## Common Pitfalls to Avoid

### Backend Pitfalls

1. **Not normalizing embeddings** → Cosine similarity won't work correctly
2. **Creating DB connection per request** → Connection pool exhaustion
3. **Not handling LLM API failures** → App crashes when Perplexity is down
4. **Hardcoding secrets** → Security vulnerability
5. **Not using async with asyncpg** → Poor performance
6. **Forgetting IVFFlat index** → Slow vector search (10-100x slower)
7. **Not batching embeddings** → Data loading takes hours

### Frontend Pitfalls

1. **Not handling loading states** → Poor UX, double submissions
2. **Using `any` in TypeScript** → Defeats purpose of types
3. **Not matching backend types** → Runtime errors
4. **Hardcoding API URL** → Breaks in production
5. **Not sanitizing user input** → XSS vulnerabilities
6. **Poor error messages** → Users don't know what went wrong

### Architecture Pitfalls

1. **Retrieving too few candidates** → LLM has limited options
2. **Retrieving too many** → Exceeds LLM context window
3. **No fallback strategy** → Complete failure on API errors
4. **Not caching embeddings** → Regenerating unnecessarily
5. **Wrong similarity metric** → Poor retrieval quality

---

## Success Criteria

Your project is interview-ready when:

✅ **Functionality**:
- Query returns relevant CPT and ICD-10 codes
- Confidence scores make sense
- Explanations are coherent
- Handles errors gracefully

✅ **Performance**:
- End-to-end latency <2.5 seconds
- Vector search <200ms
- UI responsive on mobile

✅ **Code Quality**:
- All functions have docstrings
- Comments explain design decisions
- No hardcoded values
- Type-safe throughout
- Error handling comprehensive

✅ **Documentation**:
- README clear and complete
- Setup instructions work
- Architecture explained
- Interview Q&A prepared

✅ **Professional Polish**:
- Consistent code style
- Proper Git structure
- Environment config clean
- Docker works
- Swagger docs accurate

---

## Key Differentiators for Portfolio

What makes this project stand out:

1. **Real-world RAG implementation** (not just vector search)
2. **Production patterns** (error handling, logging, async)
3. **Healthcare domain knowledge** (medical coding)
4. **Full-stack capability** (backend + frontend + database)
5. **System design thinking** (architecture docs)
6. **Scalability considerations** (documented)
7. **Cost-effective architecture** (local embeddings + serverless DB)
8. **Interview preparation** (Q&A document)

---

## Final Checklist

Before considering the project complete:

**Backend**:
- [ ] All services implemented
- [ ] Pydantic models defined
- [ ] Endpoints working
- [ ] Database schema created
- [ ] Data loaded with embeddings
- [ ] Error handling comprehensive
- [ ] Logging implemented
- [ ] Docker working

**Frontend**:
- [ ] All components built
- [ ] API client implemented
- [ ] Loading states working
- [ ] Error handling working
- [ ] Mobile responsive
- [ ] TypeScript strict mode

**Documentation**:
- [ ] README.md complete
- [ ] ARCHITECTURE.md written
- [ ] API_DOCUMENTATION.md done
- [ ] DEPLOYMENT.md created
- [ ] INTERVIEW_PREP.md finished
- [ ] EVALUATION.md written
- [ ] PROMPT_ENGINEERING.md done

**Quality**:
- [ ] All code commented
- [ ] Type hints everywhere
- [ ] No secrets in code
- [ ] Setup instructions tested
- [ ] Swagger docs verified

---

## Resources and References

**Neon Setup**:
- Enable pgvector: `CREATE EXTENSION vector;`
- Connection string format: `postgresql://user:pass@host/db`

**Sentence Transformers**:
- Model: `sentence-transformers/all-MiniLM-L6-v2`
- Dimensions: 384
- Speed: ~15ms per text on CPU

**Perplexity API**:
- Endpoint: `https://api.perplexity.ai`
- Models: `llama-3.1-sonar-large-128k-online`, `sonar-small-128k-online`
- Pricing: Check current rates

**FastAPI**:
- Auto docs: `/docs` (Swagger), `/redoc` (ReDoc)
- OpenAPI schema: `/openapi.json`

**pgvector**:
- Cosine distance: `<=>` operator
- Index type: `ivfflat` or `hnsw`
- Lists parameter: Default 100 (tunable)

---

## Next Steps After Completion

1. **Deploy to cloud** (Vercel for frontend, Railway/Render for backend)
2. **Add monitoring** (Sentry for errors, analytics)
3. **Upgrade to BioBERT** (better medical domain performance)
4. **Implement hybrid retrieval** (add BM25 keyword search)
5. **Add UMLS integration** (medical terminology expansion)
6. **Create evaluation suite** (measure accuracy)
7. **Add user authentication** (if making it public)
8. **Implement rate limiting** (protect API)
9. **Add caching layer** (Redis for frequent queries)
10. **Write comprehensive tests** (pytest, Jest)

---

## Timeline Estimate

**Minimum Viable Product**: 7-10 days
- Backend core: 3-4 days
- Frontend: 2-3 days  
- Documentation: 2-3 days

**Portfolio-Ready**: 10-14 days
- Add polish, testing, Docker, interview prep

**Production-Ready**: 20-30 days
- Add monitoring, evaluation, advanced features

---

This specification provides all mandatory requirements and implementation details without providing actual code. Use this as a blueprint to build or guide an AI agent to build the complete system.